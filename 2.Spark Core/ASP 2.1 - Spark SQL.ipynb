{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be6b5865-5779-494c-a063-c0cb7771a6f8"}}},{"cell_type":"markdown","source":["# Spark SQL\n\nDemonstrate fundamental concepts in Spark SQL using the DataFrame API.\n\n##### Objectives\n1. Run a SQL query\n1. Create a DataFrame from a table\n1. Write the same query using DataFrame transformations\n1. Trigger computation with DataFrame actions\n1. Convert between DataFrames and SQL\n\n##### Methods\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/spark_session.html\" target=\"_blank\">SparkSession</a>: **`sql`**, **`table`**\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html\" target=\"_blank\">DataFrame</a>:\n  - Transformations:  **`select`**, **`where`**, **`orderBy`**\n  - Actions: **`show`**, **`count`**, **`take`**\n  - Other methods: **`printSchema`**, **`schema`**, **`createOrReplaceTempView`**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6f8cb730-8169-408b-83c0-69007642319f"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup-SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"23968841-3b7d-430f-bfd1-0cc13e8fd49d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Python interpreter will be restarted.\nPython interpreter will be restarted.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Python interpreter will be restarted.\nPython interpreter will be restarted.\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Resetting the learning environment...\n...dropping the database \"da_sergio_salgado_4613_asp\"...(5 seconds)\n...removing the working directory \"dbfs:/mnt/dbacademy-users/sergio.salgado@n.world/apache-spark-programming-with-databricks\"...(0 seconds)\n\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03\"\n\nValidating the locally installed datasets...(3 seconds)\n\nCreating the table \"events\"...(7 seconds)\nCreating the table \"sales\"...(6 seconds)\nCreating the table \"users\"...(6 seconds)\nCreating the table \"products\"...(7 seconds)\n\nPredefined tables in \"da_sergio_salgado_4613_asp\":\n  events\n  products\n  sales\n  users\n\nPredefined paths variables:\n  DA.paths.user_db:     dbfs:/mnt/dbacademy-users/sergio.salgado@n.world/apache-spark-programming-with-databricks/database.db\n  DA.paths.datasets:    dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03\n  DA.paths.working_dir: dbfs:/mnt/dbacademy-users/sergio.salgado@n.world/apache-spark-programming-with-databricks\n  DA.paths.checkpoints: dbfs:/mnt/dbacademy-users/sergio.salgado@n.world/apache-spark-programming-with-databricks/_checkpoints\n\nSetup completed in 35 seconds\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Resetting the learning environment...\n...dropping the database \"da_sergio_salgado_4613_asp\"...(5 seconds)\n...removing the working directory \"dbfs:/mnt/dbacademy-users/sergio.salgado@n.world/apache-spark-programming-with-databricks\"...(0 seconds)\n\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03\"\n\nValidating the locally installed datasets...(3 seconds)\n\nCreating the table \"events\"...(7 seconds)\nCreating the table \"sales\"...(6 seconds)\nCreating the table \"users\"...(6 seconds)\nCreating the table \"products\"...(7 seconds)\n\nPredefined tables in \"da_sergio_salgado_4613_asp\":\n  events\n  products\n  sales\n  users\n\nPredefined paths variables:\n  DA.paths.user_db:     dbfs:/mnt/dbacademy-users/sergio.salgado@n.world/apache-spark-programming-with-databricks/database.db\n  DA.paths.datasets:    dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03\n  DA.paths.working_dir: dbfs:/mnt/dbacademy-users/sergio.salgado@n.world/apache-spark-programming-with-databricks\n  DA.paths.checkpoints: dbfs:/mnt/dbacademy-users/sergio.salgado@n.world/apache-spark-programming-with-databricks/_checkpoints\n\nSetup completed in 35 seconds\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Multiple Interfaces\nSpark SQL is a module for structured data processing with multiple interfaces.\n\nWe can interact with Spark SQL in two ways:\n1. Executing SQL queries\n1. Working with the DataFrame API."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b32f6b4d-5219-48ef-9cb1-02587f8515ef"}}},{"cell_type":"markdown","source":["**Method 1: Executing SQL queries**\n\nThis is how we interacted with Spark SQL in the previous lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"04a2177c-f238-49d5-9af5-12f1a3136388"}}},{"cell_type":"code","source":["%sql\nSELECT name, price\nFROM products\nWHERE price < 200\nORDER BY price"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b2e13698-48ce-4589-9f04-308efc67e4c5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Standard Foam Pillow",59.0],["King Foam Pillow",79.0],["Standard Down Pillow",119.0],["King Down Pillow",159.0]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"price","type":"\"double\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>price</th></tr></thead><tbody><tr><td>Standard Foam Pillow</td><td>59.0</td></tr><tr><td>King Foam Pillow</td><td>79.0</td></tr><tr><td>Standard Down Pillow</td><td>119.0</td></tr><tr><td>King Down Pillow</td><td>159.0</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Method 2: Working with the DataFrame API**\n\nWe can also express Spark SQL queries using the DataFrame API.\nThe following cell returns a DataFrame containing the same results as those retrieved above."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92e28761-3bdf-4659-95b5-d679a5affa1b"}}},{"cell_type":"code","source":["display(spark\n        .table(\"products\")\n        .select(\"name\", \"price\")\n        .where(\"price < 200\")\n        .orderBy(\"price\")\n       )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ebf4b39-9854-45e2-bd10-aebb1238651f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Standard Foam Pillow",59.0],["King Foam Pillow",79.0],["Standard Down Pillow",119.0],["King Down Pillow",159.0]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"price","type":"\"double\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>price</th></tr></thead><tbody><tr><td>Standard Foam Pillow</td><td>59.0</td></tr><tr><td>King Foam Pillow</td><td>79.0</td></tr><tr><td>Standard Down Pillow</td><td>119.0</td></tr><tr><td>King Down Pillow</td><td>159.0</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["We'll go over the syntax for the DataFrame API later in the lesson, but you can see this builder design pattern allows us to chain a sequence of operations very similar to those we find in SQL."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab21ecf6-58ae-430a-bd11-45808fcb0c71"}}},{"cell_type":"markdown","source":["## Query Execution\nWe can express the same query using any interface. The Spark SQL engine generates the same query plan used to optimize and execute on our Spark cluster.\n\n![query execution engine](https://files.training.databricks.com/images/aspwd/spark_sql_query_execution_engine.png)\n\n<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> Resilient Distributed Datasets (RDDs) are the low-level representation of datasets processed by a Spark cluster. In early versions of Spark, you had to write <a href=\"https://spark.apache.org/docs/latest/rdd-programming-guide.html\" target=\"_blank\">code manipulating RDDs directly</a>. In modern versions of Spark you should instead use the higher-level DataFrame APIs, which Spark automatically compiles into low-level RDD operations."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c92596e2-078e-4dd9-abcf-4f4b574d50cc"}}},{"cell_type":"markdown","source":["## Spark API Documentation\n\nTo learn how we work with DataFrames in Spark SQL, let's first look at the Spark API documentation.\nThe main Spark <a href=\"https://spark.apache.org/docs/latest/\" target=\"_blank\">documentation</a> page includes links to API docs and helpful guides for each version of Spark.\n\nThe <a href=\"https://spark.apache.org/docs/latest/api/scala/org/apache/spark/index.html\" target=\"_blank\">Scala API</a> and <a href=\"https://spark.apache.org/docs/latest/api/python/index.html\" target=\"_blank\">Python API</a> are most commonly used, and it's often helpful to reference the documentation for both languages.\nScala docs tend to be more comprehensive, and Python docs tend to have more code examples.\n\n#### Navigating Docs for the Spark SQL Module\nFind the Spark SQL module by navigating to **`org.apache.spark.sql`** in the Scala API or **`pyspark.sql`** in the Python API.\nThe first class we'll explore in this module is the **`SparkSession`** class. You can find this by entering \"SparkSession\" in the search bar."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"092c268b-bfb6-4b02-af06-f976628fcc74"}}},{"cell_type":"markdown","source":["## SparkSession\nThe **`SparkSession`** class is the single entry point to all functionality in Spark using the DataFrame API.\n\nIn Databricks notebooks, the SparkSession is created for you, stored in a variable called **`spark`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"953b507f-4b08-45b2-b021-24dfddae4f83"}}},{"cell_type":"code","source":["spark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b91653b3-92af-4fd3-926e-8864bfc82c81"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=2921826504397954#setting/sparkui/0927-073659-jcmtbqs5/driver-969973297804452250\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=2921826504397954#setting/sparkui/0927-073659-jcmtbqs5/driver-969973297804452250\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "]}}],"execution_count":0},{"cell_type":"markdown","source":["The example from the beginning of this lesson used the SparkSession method **`table`** to create a DataFrame from the **`products`** table. Let's save this in the variable **`products_df`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"558180cb-1fc0-44f6-86e8-75ca6b0c6e06"}}},{"cell_type":"code","source":["products_df = spark.table(\"products\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98e65134-355c-4693-ba6e-a451651679d9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Below are several additional methods we can use to create DataFrames. All of these can be found in the <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.html\" target=\"_blank\">documentation</a> for **`SparkSession`**.\n\n#### **`SparkSession`** Methods\n| Method | Description |\n| --- | --- |\n| sql | Returns a DataFrame representing the result of the given query |\n| table | Returns the specified table as a DataFrame |\n| read | Returns a DataFrameReader that can be used to read data in as a DataFrame |\n| range | Create a DataFrame with a column containing elements in a range from start to end (exclusive) with step value and number of partitions |\n| createDataFrame | Creates a DataFrame from a list of tuples, primarily used for testing |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d36f2c4d-63c3-4a9e-a00a-ea9bf0a34334"}}},{"cell_type":"markdown","source":["Let's use a SparkSession method to run SQL."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1fb7710-c886-4969-8325-b00272859d2b"}}},{"cell_type":"code","source":["result_df = spark.sql(\"\"\"\nSELECT name, price\nFROM products\nWHERE price < 200\nORDER BY price\n\"\"\")\n\ndisplay(result_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b9bb481e-3944-4e91-b255-166cdf7f174e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Standard Foam Pillow",59.0],["King Foam Pillow",79.0],["Standard Down Pillow",119.0],["King Down Pillow",159.0]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"price","type":"\"double\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>price</th></tr></thead><tbody><tr><td>Standard Foam Pillow</td><td>59.0</td></tr><tr><td>King Foam Pillow</td><td>79.0</td></tr><tr><td>Standard Down Pillow</td><td>119.0</td></tr><tr><td>King Down Pillow</td><td>159.0</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## DataFrames\nRecall that expressing our query using methods in the DataFrame API returns results in a DataFrame. Let's store this in the variable **`budget_df`**.\n\nA **DataFrame** is a distributed collection of data grouped into named columns."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11192f22-62d5-494a-9a88-1962e8f9a21b"}}},{"cell_type":"code","source":["budget_df = (spark\n             .table(\"products\")\n             .select(\"name\", \"price\")\n             .where(\"price < 200\")\n             .orderBy(\"price\")\n            )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e90f640a-1a33-4a31-8483-0a56b79afce8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We can use **`display()`** to output the results of a dataframe."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"080c707a-7281-46d6-9f4e-422401c15e2b"}}},{"cell_type":"code","source":["display(budget_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46c69255-b0fb-4d58-aaf8-d6c4272ce73e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Standard Foam Pillow",59.0],["King Foam Pillow",79.0],["Standard Down Pillow",119.0],["King Down Pillow",159.0]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"price","type":"\"double\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>price</th></tr></thead><tbody><tr><td>Standard Foam Pillow</td><td>59.0</td></tr><tr><td>King Foam Pillow</td><td>79.0</td></tr><tr><td>Standard Down Pillow</td><td>119.0</td></tr><tr><td>King Down Pillow</td><td>159.0</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["The **schema** defines the column names and types of a dataframe.\n\nAccess a dataframe's schema using the **`schema`** attribute."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57fded3b-3714-4153-99ee-8ac6393dfa10"}}},{"cell_type":"code","source":["budget_df.schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9142055a-0d54-482c-b9f7-7bc980f3406d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[11]: StructType(List(StructField(name,StringType,true),StructField(price,DoubleType,true)))","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[11]: StructType(List(StructField(name,StringType,true),StructField(price,DoubleType,true)))"]}}],"execution_count":0},{"cell_type":"markdown","source":["View a nicer output for this schema using the **`printSchema()`** method."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1b0792d-174c-4227-8125-00bfef8131a5"}}},{"cell_type":"code","source":["budget_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ccb80d93-35d0-47a2-8db1-296788306580"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name: string (nullable = true)\n |-- price: double (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: string (nullable = true)\n |-- price: double (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Transformations\nWhen we created **`budget_df`**, we used a series of DataFrame transformation methods e.g. **`select`**, **`where`**, **`orderBy`**.\n\n<strong><code>products_df  \n&nbsp;  .select(\"name\", \"price\")  \n&nbsp;  .where(\"price < 200\")  \n&nbsp;  .orderBy(\"price\")  \n</code></strong>\n    \nTransformations operate on and return DataFrames, allowing us to chain transformation methods together to construct new DataFrames.\nHowever, these operations can't execute on their own, as transformation methods are **lazily evaluated**.\n\nRunning the following cell does not trigger any computation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"131636f2-952a-4d4d-abe7-40497cdf9dd3"}}},{"cell_type":"code","source":["(products_df\n  .select(\"name\", \"price\")\n  .where(\"price < 200\")\n  .orderBy(\"price\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f9eec15c-6297-47f7-a1bd-cac76c3413fb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[13]: DataFrame[name: string, price: double]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[13]: DataFrame[name: string, price: double]"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Actions\nConversely, DataFrame actions are methods that **trigger computation**.\nActions are needed to trigger the execution of any DataFrame transformations.\n\nThe **`show`** action causes the following cell to execute transformations."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68281151-e7be-4161-9b5f-1f4ee9e9f9df"}}},{"cell_type":"code","source":["(products_df\n  .select(\"name\", \"price\")\n  .where(\"price < 200\")\n  .orderBy(\"price\")\n  .show())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6d5eabde-998e-4b2d-b67a-785e8b3e3c99"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+-----+\n|                name|price|\n+--------------------+-----+\n|Standard Foam Pillow| 59.0|\n|    King Foam Pillow| 79.0|\n|Standard Down Pillow|119.0|\n|    King Down Pillow|159.0|\n+--------------------+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+-----+\n|                name|price|\n+--------------------+-----+\n|Standard Foam Pillow| 59.0|\n|    King Foam Pillow| 79.0|\n|Standard Down Pillow|119.0|\n|    King Down Pillow|159.0|\n+--------------------+-----+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Below are several examples of <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#dataframe-apis\" target=\"_blank\">DataFrame</a> actions.\n\n### DataFrame Action Methods\n| Method | Description |\n| --- | --- |\n| show | Displays the top n rows of DataFrame in a tabular form |\n| count | Returns the number of rows in the DataFrame |\n| describe,  summary | Computes basic statistics for numeric and string columns |\n| first, head | Returns the the first row |\n| collect | Returns an array that contains all rows in this DataFrame |\n| take | Returns an array of the first n rows in the DataFrame |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f471239-d4b3-4192-b31e-e67485dccb10"}}},{"cell_type":"markdown","source":["**`count`** returns the number of records in a DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e8b12d5-0c0c-45a4-b2e5-a01392c2de4f"}}},{"cell_type":"code","source":["budget_df.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"39996852-d0de-4636-9df8-daa87c286c96"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[15]: 4","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[15]: 4"]}}],"execution_count":0},{"cell_type":"markdown","source":["**`collect`** returns an array of all rows in a DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0df17ce-c1df-4e5f-8846-b1b5e5b97dc5"}}},{"cell_type":"code","source":["budget_df.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b17392e2-b5ed-4a9c-9b2c-5db46d918495"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[16]: [Row(name='Standard Foam Pillow', price=59.0),\n Row(name='King Foam Pillow', price=79.0),\n Row(name='Standard Down Pillow', price=119.0),\n Row(name='King Down Pillow', price=159.0)]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[16]: [Row(name='Standard Foam Pillow', price=59.0),\n Row(name='King Foam Pillow', price=79.0),\n Row(name='Standard Down Pillow', price=119.0),\n Row(name='King Down Pillow', price=159.0)]"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Convert between DataFrames and SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f69db2c1-0bc9-4b5f-ad23-2561f5a8d670"}}},{"cell_type":"markdown","source":["**`createOrReplaceTempView`** creates a temporary view based on the DataFrame. The lifetime of the temporary view is tied to the SparkSession that was used to create the DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c57e8cff-5212-4431-8041-32f98ccabdd5"}}},{"cell_type":"code","source":["budget_df.createOrReplaceTempView(\"budget\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68c12db2-32d4-4b70-a419-b0e799081288"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(spark.sql(\"SELECT * FROM budget\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24634a6a-4c68-498e-be12-78298faf7888"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Standard Foam Pillow",59.0],["King Foam Pillow",79.0],["Standard Down Pillow",119.0],["King Down Pillow",159.0]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"price","type":"\"double\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>price</th></tr></thead><tbody><tr><td>Standard Foam Pillow</td><td>59.0</td></tr><tr><td>King Foam Pillow</td><td>79.0</td></tr><tr><td>Standard Down Pillow</td><td>119.0</td></tr><tr><td>King Down Pillow</td><td>159.0</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Classroom Cleanup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fef29489-d7f0-4c1e-84ca-c7d4573b12c3"}}},{"cell_type":"code","source":["DA.cleanup()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4bfd1e87-fb4d-41f6-9cc1-1e43ddd75af5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Resetting the learning environment...\n...dropping the database \"da_sergio_salgado_4613_asp\"...(2 seconds)\n...removing the working directory \"dbfs:/mnt/dbacademy-users/sergio.salgado@n.world/apache-spark-programming-with-databricks\"...(0 seconds)\n\nValidating the locally installed datasets...(3 seconds)\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Resetting the learning environment...\n...dropping the database \"da_sergio_salgado_4613_asp\"...(2 seconds)\n...removing the working directory \"dbfs:/mnt/dbacademy-users/sergio.salgado@n.world/apache-spark-programming-with-databricks\"...(0 seconds)\n\nValidating the locally installed datasets...(3 seconds)\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46ee58d4-b2fc-4728-8539-c6a1062459cf"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ASP 2.1 - Spark SQL","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3111103097067951}},"nbformat":4,"nbformat_minor":0}
