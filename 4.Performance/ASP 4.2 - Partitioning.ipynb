{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00f31ab2-0b93-45fd-b1ad-258abe55621c"}}},{"cell_type":"markdown","source":["# Partitioning\n##### Objectives\n1. Get partitions and cores\n1. Repartition DataFrames\n1. Configure default shuffle partitions\n\n##### Methods\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html\" target=\"_blank\">DataFrame</a>: **`repartition`**, **`coalesce`**, **`rdd.getNumPartitions`**\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkConf.html\" target=\"_blank\">SparkConf</a>: **`get`**, **`set`**\n- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.html\" target=\"_blank\">SparkSession</a>: **`spark.sparkContext.defaultParallelism`**\n\n##### SparkConf Parameters\n- **`spark.sql.shuffle.partitions`**, **`spark.sql.adaptive.enabled`**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19ac76ea-59bf-41ef-ad1a-3a58bcfdaea0"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"579f5884-e371-4283-870b-0f6e3a35a3a1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Python interpreter will be restarted.\nPython interpreter will be restarted.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Python interpreter will be restarted.\nPython interpreter will be restarted.\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03\"\n\nValidating the locally installed datasets...(5 seconds)\n\nPredefined tables in \"da_sergio_salgado_4613_asp\":\n  -none-\n\nPredefined paths variables:\n  DA.paths.user_db:     dbfs:/mnt/dbacademy-users/sergio.salgado@n.world/apache-spark-programming-with-databricks/database.db\n  DA.paths.datasets:    dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03\n  DA.paths.working_dir: dbfs:/mnt/dbacademy-users/sergio.salgado@n.world/apache-spark-programming-with-databricks\n  DA.paths.checkpoints: dbfs:/mnt/dbacademy-users/sergio.salgado@n.world/apache-spark-programming-with-databricks/_checkpoints\n  DA.paths.sales:       dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03/ecommerce/sales/sales.delta\n  DA.paths.users:       dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03/ecommerce/users/users.delta\n  DA.paths.events:      dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03/ecommerce/events/events.delta\n  DA.paths.products:    dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03/products/products.delta\n\nSetup completed in 6 seconds\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03\"\n\nValidating the locally installed datasets...(5 seconds)\n\nPredefined tables in \"da_sergio_salgado_4613_asp\":\n  -none-\n\nPredefined paths variables:\n  DA.paths.user_db:     dbfs:/mnt/dbacademy-users/sergio.salgado@n.world/apache-spark-programming-with-databricks/database.db\n  DA.paths.datasets:    dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03\n  DA.paths.working_dir: dbfs:/mnt/dbacademy-users/sergio.salgado@n.world/apache-spark-programming-with-databricks\n  DA.paths.checkpoints: dbfs:/mnt/dbacademy-users/sergio.salgado@n.world/apache-spark-programming-with-databricks/_checkpoints\n  DA.paths.sales:       dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03/ecommerce/sales/sales.delta\n  DA.paths.users:       dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03/ecommerce/users/users.delta\n  DA.paths.events:      dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03/ecommerce/events/events.delta\n  DA.paths.products:    dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03/products/products.delta\n\nSetup completed in 6 seconds\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Get partitions and cores\n\nUse the **`rdd`** method **`getNumPartitions`** to get the number of DataFrame partitions."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03975c9d-086e-4d41-96af-4a74ac719c09"}}},{"cell_type":"code","source":["df = spark.read.format(\"delta\").load(DA.paths.events)\ndf.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7034035-7ccf-4e76-9b33-880579d99296"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[4]: 4","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[4]: 4"]}}],"execution_count":0},{"cell_type":"markdown","source":["Access **`SparkContext`** through **`SparkSession`** to get the number of cores or slots.\n\nUse the **`defaultParallelism`** attribute to get the number of cores in a cluster."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50460f4f-f6eb-402f-a2d5-bca683bbb97a"}}},{"cell_type":"code","source":["print(spark.sparkContext.defaultParallelism)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"449efff9-6f4d-411e-98f6-d0192158be9b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"8\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["8\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["**`SparkContext`** is also provided in Databricks notebooks as the variable **`sc`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f95825b7-b308-454d-9ab9-2b9f60f941ca"}}},{"cell_type":"code","source":["print(sc.defaultParallelism)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bcd401ef-3063-4bea-b37c-d3045f387eb0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"8\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["8\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Repartition DataFrame\n\nThere are two methods available to repartition a DataFrame: **`repartition`** and **`coalesce`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"351f1e15-e0b0-4851-8f85-ad88f886318b"}}},{"cell_type":"markdown","source":["#### **`repartition`**\nReturns a new DataFrame that has exactly **`n`** partitions.\n\n- Wide transformation\n- Pro: Evenly balances partition sizes  \n- Con: Requires shuffling all data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d59a8d1c-31ab-497d-b356-4d026b15aa3a"}}},{"cell_type":"code","source":["repartitioned_df = df.repartition(8)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56a4be18-99ca-45d4-a4fa-4b2795e724fb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["repartitioned_df.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fd48d3cc-ae4e-4e9b-ad77-c25513f8d9ca"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[8]: 8","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[8]: 8"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### **`coalesce`**\nReturns a new DataFrame that has exactly **`n`** partitions, when fewer partitions are requested.\n\nIf a larger number of partitions is requested, it will stay at the current number of partitions.\n\n- Narrow transformation, some partitions are effectively concatenated\n- Pro: Requires no shuffling\n- Cons:\n  - Is not able to increase # partitions\n  - Can result in uneven partition sizes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3984c5be-7125-475d-9c1e-b0056af1f437"}}},{"cell_type":"code","source":["coalesce_df = df.coalesce(8)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"211294b9-b962-4881-89af-96c0d32b6bc2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["coalesce_df.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a436200-3ba0-422c-85e8-a0ccf04414b8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[18]: 4","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[18]: 4"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Configure default shuffle partitions\n\nUse the SparkSession's **`conf`** attribute to get and set dynamic Spark configuration properties. The **`spark.sql.shuffle.partitions`** property determines the number of partitions that result from a shuffle. Let's check its default value:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ccbb1319-b612-4729-9640-7e45b7b7acb4"}}},{"cell_type":"code","source":["spark.conf.get(\"spark.sql.shuffle.partitions\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb00658c-30ed-4c28-a3e2-578937fa82ee"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[19]: '200'","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[19]: '200'"]}}],"execution_count":0},{"cell_type":"markdown","source":["Assuming that the data set isn't too large, you could configure the default number of shuffle partitions to match the number of cores:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6045d90b-84f6-4533-a3d9-9d4af5f45b43"}}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)\nprint(spark.conf.get(\"spark.sql.shuffle.partitions\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b2256a3b-2a49-4bcc-94e5-2afab669eb3b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"8\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["8\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Partitioning Guidelines\n- Make the number of partitions a multiple of the number of cores\n- Target a partition size of ~200MB\n- Size default shuffle partitions by dividing largest shuffle stage input by the target partition size (e.g., 4TB / 200MB = 20,000 shuffle partition count)\n\n<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> When writing a DataFrame to storage, the number of DataFrame partitions determines the number of data files written. (This assumes that <a href=\"https://sparkbyexamples.com/apache-hive/hive-partitions-explained-with-examples/\" target=\"_blank\">Hive partitioning</a> is not used for the data in storage. A discussion of DataFrame partitioning vs Hive partitioning is beyond the scope of this class.)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc9805f1-cf3d-462c-907b-a73e10df1964"}}},{"cell_type":"markdown","source":["### Adaptive Query Execution\n\n<img src=\"https://files.training.databricks.com/images/aspwd/partitioning_aqe.png\" width=\"60%\" />\n\nIn Spark 3, <a href=\"https://spark.apache.org/docs/latest/sql-performance-tuning.html#adaptive-query-execution\" target=\"_blank\">AQE</a> is now able to <a href=\"https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html\" target=\"_blank\"> dynamically coalesce shuffle partitions</a> at runtime. This means that you can set **`spark.sql.shuffle.partitions`** based on the largest data set your application processes and allow AQE to reduce the number of partitions automatically when there is less data to process.\n\nThe **`spark.sql.adaptive.enabled`** configuration option controls whether AQE is turned on/off."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f30484f4-39e5-4681-ae43-0814d6094b03"}}},{"cell_type":"code","source":["spark.conf.get(\"spark.sql.adaptive.enabled\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c772afc7-eba6-4555-a968-3c9da5537b9f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[21]: 'true'","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[21]: 'true'"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Clean up classroom"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60765d2a-a1a8-4406-beea-d2de64c016ab"}}},{"cell_type":"code","source":["DA.cleanup()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f836a0fa-5731-4c51-bec2-0e6d0aed8576"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Resetting the learning environment...\n...dropping the database \"da_sergio_salgado_4613_asp\"...(1 seconds)\n...removing the working directory \"dbfs:/mnt/dbacademy-users/sergio.salgado@n.world/apache-spark-programming-with-databricks\"...(0 seconds)\n\nValidating the locally installed datasets...(5 seconds)\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Resetting the learning environment...\n...dropping the database \"da_sergio_salgado_4613_asp\"...(1 seconds)\n...removing the working directory \"dbfs:/mnt/dbacademy-users/sergio.salgado@n.world/apache-spark-programming-with-databricks\"...(0 seconds)\n\nValidating the locally installed datasets...(5 seconds)\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5a9e70b-8fea-4541-801c-12211dedfc05"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ASP 4.2 - Partitioning","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3111103097067383}},"nbformat":4,"nbformat_minor":0}
