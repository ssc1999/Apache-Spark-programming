{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc1ddd0f-9624-4f3c-b596-0dd8d8467866"}}},{"cell_type":"markdown","source":["## Hourly Activity by Traffic Lab\nProcess streaming data to display the total active users by traffic source with a 1 hour window.\n1. Cast to timestamp and add watermark for 2 hours\n2. Aggregate active users by traffic source for 1 hour windows\n3. Execute query with **`display`** and plot results\n5. Use query name to stop streaming query"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"69b29027-bd43-4992-9464-7e14d9e538be"}}},{"cell_type":"markdown","source":["### Setup\nRun the cells below to generate hourly JSON files of event data for July 3, 2020."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61b82883-029c-4bb5-bd36-3646d9804079"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ec5d78f-cd3e-46c1-bb0b-a45b07c15f91"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Python interpreter will be restarted.\nPython interpreter will be restarted.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Python interpreter will be restarted.\nPython interpreter will be restarted.\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Resetting the learning environment...\n...dropping the database \"da_sergio_salgado_4613_asp\"...(0 seconds)\n...removing the working directory \"dbfs:/mnt/dbacademy-users/sergio.salgado@n.world/apache-spark-programming-with-databricks\"...(0 seconds)\n\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03\"\n\nValidating the locally installed datasets...(5 seconds)\n\nPredefined tables in \"da_sergio_salgado_4613_asp\":\n  -none-\n\nPredefined paths variables:\n  DA.paths.user_db:     dbfs:/mnt/dbacademy-users/sergio.salgado@n.world/apache-spark-programming-with-databricks/database.db\n  DA.paths.datasets:    dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03\n  DA.paths.working_dir: dbfs:/mnt/dbacademy-users/sergio.salgado@n.world/apache-spark-programming-with-databricks\n  DA.paths.checkpoints: dbfs:/mnt/dbacademy-users/sergio.salgado@n.world/apache-spark-programming-with-databricks/_checkpoints\n  DA.paths.sales:       dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03/ecommerce/sales/sales.delta\n  DA.paths.users:       dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03/ecommerce/users/users.delta\n  DA.paths.events:      dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03/ecommerce/events/events.delta\n  DA.paths.products:    dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03/products/products.delta\n\nSetup completed in 7 seconds\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Resetting the learning environment...\n...dropping the database \"da_sergio_salgado_4613_asp\"...(0 seconds)\n...removing the working directory \"dbfs:/mnt/dbacademy-users/sergio.salgado@n.world/apache-spark-programming-with-databricks\"...(0 seconds)\n\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03\"\n\nValidating the locally installed datasets...(5 seconds)\n\nPredefined tables in \"da_sergio_salgado_4613_asp\":\n  -none-\n\nPredefined paths variables:\n  DA.paths.user_db:     dbfs:/mnt/dbacademy-users/sergio.salgado@n.world/apache-spark-programming-with-databricks/database.db\n  DA.paths.datasets:    dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03\n  DA.paths.working_dir: dbfs:/mnt/dbacademy-users/sergio.salgado@n.world/apache-spark-programming-with-databricks\n  DA.paths.checkpoints: dbfs:/mnt/dbacademy-users/sergio.salgado@n.world/apache-spark-programming-with-databricks/_checkpoints\n  DA.paths.sales:       dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03/ecommerce/sales/sales.delta\n  DA.paths.users:       dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03/ecommerce/users/users.delta\n  DA.paths.events:      dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03/ecommerce/events/events.delta\n  DA.paths.products:    dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03/products/products.delta\n\nSetup completed in 7 seconds\n"]}}],"execution_count":0},{"cell_type":"code","source":["schema = \"device STRING, ecommerce STRUCT<purchase_revenue_in_usd: DOUBLE, total_item_quantity: BIGINT, unique_items: BIGINT>, event_name STRING, event_previous_timestamp BIGINT, event_timestamp BIGINT, geo STRUCT<city: STRING, state: STRING>, items ARRAY<STRUCT<coupon: STRING, item_id: STRING, item_name: STRING, item_revenue_in_usd: DOUBLE, price_in_usd: DOUBLE, quantity: BIGINT>>, traffic_source STRING, user_first_touch_timestamp BIGINT, user_id STRING\"\n\n# Directory of hourly events logged from the BedBricks website on July 3, 2020\nhourly_events_path = f\"{DA.paths.datasets}/ecommerce/events/events-2020-07-03.json\"\n\ndf = (spark\n      .readStream\n      .schema(schema)\n      .option(\"maxFilesPerTrigger\", 1)\n      .json(hourly_events_path)\n     )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61f4e770-4d23-4ad5-b98f-c4c3e224a090"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 1. Cast to timestamp and add watermark for 2 hours\n- Add a **`createdAt`** column by dividing **`event_timestamp`** by 1M and casting to timestamp\n- Set a watermark of 2 hours on the **`createdAt`** column\n\nAssign the resulting DataFrame to **`events_df`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"251a9ac2-d88f-4f97-9f06-a0a59e40148e"}}},{"cell_type":"code","source":["# TODO\nfrom pyspark.sql.functions import col\nevents_df = (df\n             .withColumn(\"createdAt\", (col(\"event_timestamp\") / 1e6).cast(\"timestamp\"))\n             .withWatermark(\"createdAt\", \"2 hours\") \n            )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8be3d49b-61c7-47e5-b8d0-606a3ac21a2d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**1.1: CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19b78426-9b58-44ec-a97a-1a6b245334c7"}}},{"cell_type":"code","source":["assert \"StructField(createdAt,TimestampType,true\" in str(events_df.schema)\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ece4e6fb-f438-426f-81c5-f92d9d6fbb9b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"All test pass\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["All test pass\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 2. Aggregate active users by traffic source for 1 hour windows\n\n- Set the default shuffle partitions to the number of cores on your cluster\n- Group by **`traffic_source`** with 1-hour tumbling windows based on the **`createdAt`** column\n- Aggregate the approximate count of distinct users per **`user_id`** and alias the resulting column to **`active_users`**\n- Select **`traffic_source`**, **`active_users`**, and the **`hour`** extracted from **`window.start`** with an alias of **`hour`**\n- Sort by **`hour`** in ascending order\nAssign the resulting DataFrame to **`traffic_df`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7bd7560a-45b7-44d0-90bc-5c247884d273"}}},{"cell_type":"code","source":["# TODO\nfrom pyspark.sql.functions import approx_count_distinct, hour, window\n\nspark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)\n\ntraffic_df = (events_df\n              .groupBy(\"traffic_source\", window(col(\"traffic_source\"), \"1 hour\"))\n              .agg(approx_count_distinct(\"user_id\").alias(\"active_users\"))\n              .select(\"traffic_source\", \"active_users\", hour(col(\"window.start\")).alias(\"hour\"))\n              .sort(\"hour\")\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0832c421-5df2-42d3-b51e-5b80ecea33cb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**2.1: CHECK YOUR WORK**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7e2aee25-2475-4e8f-9dc9-c02842c2d5f7"}}},{"cell_type":"code","source":["assert str(traffic_df.schema) == \"StructType(List(StructField(traffic_source,StringType,true),StructField(active_users,LongType,false),StructField(hour,IntegerType,true)))\"\nprint(\"All test pass\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aaae470a-da3b-4877-8951-7e50d00fd696"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"All test pass\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["All test pass\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 3. Execute query with display() and plot results\n- Use **`display`** to start **`traffic_df`** as a streaming query and display the resulting memory sink\n  - Assign \"hourly_traffic\" as the name of the query by setting the **`streamName`** parameter of **`display`**\n- Plot the streaming query results as a bar graph\n- Configure the following plot options:\n  - Keys: **`hour`**\n  - Series groupings: **`traffic_source`**\n  - Values: **`active_users`**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de163705-966d-473c-ab3a-7584322258f0"}}},{"cell_type":"code","source":["# TODO\ndisplay(traffic_df, streamName=\"hourly_traffic\")\ntraffic_df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc8ad00f-e5c2-4f8a-98ec-b62e1a313cc8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"traffic_source","type":"\"string\"","metadata":"{}"},{"name":"active_users","type":"\"long\"","metadata":"{}"},{"name":"hour","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>traffic_source</th><th>active_users</th><th>hour</th></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**3.1: CHECK YOUR WORK**\n\n- The bar chart should plot **`hour`** on the x-axis and **`active_users`** on the y-axis\n- Six bars should appear at every hour for all traffic sources\n- The chart should stop at hour 23"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1fc2e409-13c9-463e-a98a-67a768267e7e"}}},{"cell_type":"markdown","source":["### 4. Manage streaming query\n- Iterate over SparkSession's list of active streams to find one with name \"hourly_traffic\"\n- Stop the streaming query"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"55d89e1b-5a7f-437c-a670-dc37ebb40070"}}},{"cell_type":"code","source":["# TODO\nDA.block_until_stream_is_ready(\"hourly_traffic\")\n\nfor s in spark.streams.active:\n  if s.name == \"hourly_traffic\":\n    s.stop()\n    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6fe4cd16-29ef-4ee6-b141-664f7d453d02"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Processed 0 of 2 batches...\nProcessed 0 of 2 batches...\nProcessed 2 of 2 batches...\nThe stream is now active with 2 batches having been processed.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Processed 0 of 2 batches...\nProcessed 0 of 2 batches...\nProcessed 2 of 2 batches...\nThe stream is now active with 2 batches having been processed.\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["**4.1: CHECK YOUR WORK**\nPrint all active streams to check that \"hourly_traffic\" is no longer there"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af8b6f71-9ab8-44a8-8407-cd486c4cce73"}}},{"cell_type":"code","source":["for s in spark.streams.active:\n    print(s.name)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ada4b23b-940b-48ab-9e2c-531d2783b3f9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Classroom Cleanup\nRun the cell below to clean up resources."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1a88d115-b483-4bfe-9ef3-45df30055bfc"}}},{"cell_type":"code","source":["DA.cleanup()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b1e9a1a-37be-4ed7-9ad0-8e6733c6359f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Resetting the learning environment...\n...dropping the database \"da_sergio_salgado_4613_asp\"...(0 seconds)\n...removing the working directory \"dbfs:/mnt/dbacademy-users/sergio.salgado@n.world/apache-spark-programming-with-databricks\"...(0 seconds)\n\nValidating the locally installed datasets...(4 seconds)\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Resetting the learning environment...\n...dropping the database \"da_sergio_salgado_4613_asp\"...(0 seconds)\n...removing the working directory \"dbfs:/mnt/dbacademy-users/sergio.salgado@n.world/apache-spark-programming-with-databricks\"...(0 seconds)\n\nValidating the locally installed datasets...(4 seconds)\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10a19ab2-0bf0-4729-99ab-4ddbf96f329b"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ASP 5.1bL - Hourly Activity by Traffic Lab","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3111103097067712}},"nbformat":4,"nbformat_minor":0}
